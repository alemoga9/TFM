{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNY6rb4FSa2N2u3VUVUvd8X"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"GgkYF0JHjgn-"},"source":["# SRGAN"]},{"cell_type":"code","metadata":{"id":"g6xisFCXjO3M"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.models as models\n","from torch import Tensor\n","\n","class ResidualConvBlock(nn.Module):\n","    \"\"\"Bloques convoluciones residuales.\n","    Args:\n","        channels (int): número de canales de la imagen de entrada.\n","    \"\"\"\n","    def __init__(self, channels):\n","        super(ResidualConvBlock, self).__init__()\n","        self.rc_block = nn.Sequential(\n","            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(channels),\n","            nn.PReLU(),\n","            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(channels)\n","        )\n","\n","    def forward(self, x):\n","        out = self.rc_block(x)\n","        out = out + x\n","        return out\n","\n","class Discriminator(nn.Module):\n","    def __init__(self) -> None:\n","        super(Discriminator, self).__init__()\n","        self.features = nn.Sequential(\n","            # input size. (3) x 256 x 256\n","            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=True),\n","            nn.LeakyReLU(0.2, True),\n","\n","            # state size. (64) x 128 x 128\n","            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2, True),\n","\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, True),\n","\n","            # state size. (128) x 64 x 64\n","            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, True),\n","\n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, True),\n","\n","            # state size. (256) x 32 x 32\n","            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, True),\n","\n","            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2, True),\n","\n","            # state size. (512) x 16 x 16\n","            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2, True)\n","        )\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512*16*16, 1024),\n","            nn.LeakyReLU(0.2, True),\n","            nn.Linear(1024, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        out = self.features(x)\n","        out = torch.flatten(out, 1)\n","        out = self.classifier(out)\n","\n","        return out\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","\n","        # Primera capa convolucional\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=9, stride=1, padding=4), \n","            nn.PReLU()\n","            )\n","\n","        # Bloques residuales\n","        res_blocks = []\n","        for _ in range(16):\n","            res_blocks.append(ResidualConvBlock(64))\n","        self.res_blocks = nn.Sequential(*res_blocks)\n","\n","        # Segunda capa convolucional\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False), \n","            nn.BatchNorm2d(64)\n","            )\n","\n","        # Bloque convolución upsampling (x4)\n","        self.upsampling = nn.Sequential(\n","            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),\n","            nn.PixelShuffle(2),\n","            nn.PReLU(),\n","            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),\n","            nn.PixelShuffle(2),\n","            nn.PReLU()\n","        )\n","\n","        # Capa de salida\n","        self.conv3 = nn.Conv2d(64, 3, kernel_size=9, stride=1, padding=4)\n","\n","        # Se inicializa los pesos \n","        self.initialize_weights()  \n","\n","    def forward(self, x):\n","        out1 = self.conv1(x)\n","        out = self.res_blocks(out1)\n","        out2 = self.conv2(out)\n","        out = out1 + out2\n","        out = self.upsampling(out)\n","        out = self.conv3(out)\n","        return out\n","\n","    def initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight)\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","                m.weight.data *= 0.1\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                m.weight.data *= 0.1\n","\n","class ContentLoss(nn.Module):\n","    \"\"\"Se construye una función de pérdida de contenido basada en la red VGG19.\n","       El uso de capas de mapeo de características de alto nivel de las últimas \n","       capas se centrará más en el contenido de textura de la imagen.\n","    \"\"\"\n","\n","    def __init__(self) -> None:\n","        super(ContentLoss, self).__init__()\n","\n","        # Se carga el modelo VGG19 entrenado en el dataset ImageNet.\n","        vgg19 = models.vgg19(pretrained=True, num_classes=1000).eval()\n","\n","        # Se extrae la salida de la capa 36 del modelo VGG19 como pérdida de contenido.\n","        self.feature_extractor = nn.Sequential(*list(vgg19.features.children())[:36])\n","\n","        # Se congelan los parámetros del modelo.\n","        for parameters in self.feature_extractor.parameters():\n","            parameters.requires_grad = False\n","\n","    def forward(self, sr, hr):\n","        # Pérdida basada en las diferencias de los mapas de características de \n","        # ambas imágenes.\n","        loss = F.mse_loss(self.feature_extractor(sr), self.feature_extractor(hr))\n","\n","        return loss"],"execution_count":null,"outputs":[]}]}