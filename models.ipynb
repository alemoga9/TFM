{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"models.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMdZKhgswOg2xgQd8M1BacL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"77fcmjqJ1lXj"},"source":["##UNet"]},{"cell_type":"code","metadata":{"id":"MEXR5eCw0jrW"},"source":["import torch\n","import torch.nn.functional as F\n","\n","\n","def conv3x3_bn(ci, co):\n","    return torch.nn.Sequential(\n","        # capa convolucional con filtro 3x3 y padding = 1, por lo que no cambian las dimensiones\n","        torch.nn.Conv2d(ci, co, 3, padding=1), # ci = canales de entrada; co 0 canales de salida\n","        torch.nn.BatchNorm2d(co),\n","        torch.nn.ReLU(inplace=True)\n","    )\n","\n","def encoder_conv(ci, co):\n","  return torch.nn.Sequential(\n","        torch.nn.MaxPool2d(2),  # max pool 2x2\n","        conv3x3_bn(ci, co),     # conv 3x3, ReLU\n","        conv3x3_bn(co, co),     # conv 3x3, ReLU\n","    )\n","\n","class deconv(torch.nn.Module):\n","    def __init__(self, ci, co):\n","        super(deconv, self).__init__()\n","\n","        # upconv 2x2 (convolución traspuesta 2D)\n","        self.upsample = torch.nn.ConvTranspose2d(ci, co, 2, stride=2) # kernel de tamaño 2x2 y stride = 2 \n","        # con kernel 2x2 y stride = 2 se obtiene un mapa de características del doble de resolución\n","\n","        self.conv1 = conv3x3_bn(ci, co) # conv 3x3, ReLU\n","        self.conv2 = conv3x3_bn(co, co) # conv 3x3, ReLU\n","    \n","    # recibe la salida de la capa anterior(x1) y la salida de la etapa\n","    # correspondiente del encoder (x2)\n","    def forward(self, x1, x2):\n","        x1 = self.upsample(x1)  # A la salida de la capa anterior se aplica convolución traspuesta\n","\n","        # Para evitar inconcordancias en el tamaño y que las dimensiones sean las mismas\n","        diffX = x2.size()[2] - x1.size()[2]\n","        diffY = x2.size()[3] - x1.size()[3]\n","        x1 = F.pad(x1, (diffX, 0, diffY, 0))\n","\n","        # concatenamos los tensores\n","        x = torch.cat([x2, x1], dim=1)\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        return x\n","\n","\n","# n_classes número de clases diferentes\n","# in_ch número de canales de la imagen de entrada\n","class UNet(torch.nn.Module):\n","    def __init__(self, n_classes=3, in_ch=1): \n","        super().__init__()\n","\n","        # lista de capas en encoder-decoder con número de filtros\n","        c = [16, 32, 64, 128] # enconder en este orden; decoder en orden inverso\n","\n","        # primera capa conv que recibe la imagen\n","        self.conv1 = torch.nn.Sequential(\n","          conv3x3_bn(in_ch, c[0]),\n","          conv3x3_bn(c[0], c[0]),\n","        )\n","        # capas del encoder\n","        self.conv2 = encoder_conv(c[0], c[1])\n","        self.conv3 = encoder_conv(c[1], c[2])\n","        self.conv4 = encoder_conv(c[2], c[3])\n","\n","        # capas del decoder\n","        self.deconv1 = deconv(c[3],c[2])\n","        self.deconv2 = deconv(c[2],c[1])\n","        self.deconv3 = deconv(c[1],c[0])\n","\n","        # útlima capa conv que nos da la máscara\n","        # el número de filtros a la entrada es el primer valor de la lista\n","        # se aplican tantos filtros como clases haya \n","        self.out = torch.nn.Conv2d(c[0], n_classes, 3, padding=1)\n","\n","    def forward(self, x):\n","        # encoder\n","        x1 = self.conv1(x)\n","        x2 = self.conv2(x1)\n","        x3 = self.conv3(x2)\n","        x = self.conv4(x3)\n","        # decoder\n","        x = self.deconv1(x, x3)\n","        x = self.deconv2(x, x2)\n","        x = self.deconv3(x, x1)\n","        x = self.out(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KbAHP0-91qO9"},"source":["##UNet-ResNet"]},{"cell_type":"code","metadata":{"id":"XagXZhsH1pMq"},"source":["class out_conv(torch.nn.Module):\n","    def __init__(self, ci, co, coo):\n","        super(out_conv, self).__init__()\n","\n","        # upconv 2x2 (convolución traspuesta 2D)\n","        self.upsample = torch.nn.ConvTranspose2d(ci, co, 2, stride=2) # kernel de tamaño 2x2 y stride = 2\n","        # con kernel 2x2 y stride = 2 se obtiene un mapa de características del doble de resolución\n","\n","        self.conv = conv3x3_bn(ci, co)  # conv 3x3, ReLU\n","        self.final = torch.nn.Conv2d(co, coo, 1) # A la salida se obtiene el número de clases necesario\n","\n","    def forward(self, x1, x2):\n","        x1 = self.upsample(x1)\n","        diffX = x2.size()[2] - x1.size()[2]\n","        diffY = x2.size()[3] - x1.size()[3]\n","        x1 = F.pad(x1, (diffX, 0, diffY, 0))\n","        x = self.conv(x1)\n","        x = self.final(x)\n","        return x\n","\n","class UNetResnet(torch.nn.Module):\n","    def __init__(self, n_classes=3, in_ch=1):\n","        super().__init__()\n","\n","        # En lugar de utilizar un encoder propio, se utiliza una red entrenada: ResNet18\n","        self.encoder = torchvision.models.resnet18(pretrained=True)  \n","\n","        # ResNet18 está diseñada para trabajar con 3 canaless, pero nuestras imagenes son en ByN \n","        # por lo que es necesasrio modificar la primera capa    \n","        if in_ch != 3:\n","          self.encoder.conv1 = torch.nn.Conv2d(in_ch, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","\n","\n","        self.deconv1 = deconv(512,256)\n","        self.deconv2 = deconv(256,128)\n","        self.deconv3 = deconv(128,64)\n","\n","        # Capa que genera los mapas de salidas. Estos tienen que tener tantos canales \n","        # como número de clases\n","        self.out = out_conv(64, 64, n_classes)\n","\n","\n","    def forward(self, x):\n","        x_in = torch.tensor(x.clone())\n","        x = self.encoder.relu(self.encoder.bn1(self.encoder.conv1(x)))\n","        x1 = self.encoder.layer1(x)\n","        x2 = self.encoder.layer2(x1)\n","        x3 = self.encoder.layer3(x2)\n","        x = self.encoder.layer4(x3)\n","        x = self.deconv1(x, x3)\n","        x = self.deconv2(x, x2)\n","        x = self.deconv3(x, x1)\n","        x = self.out(x, x_in)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WGFG9_PJdUC4"},"source":["# VDSR"]},{"cell_type":"code","metadata":{"id":"qnRftTUPdTaI"},"source":["import torch\n","import torch.nn as nn\n","from math import sqrt\n","\n","class Conv_ReLU_Block(nn.Module):\n","    def __init__(self):\n","        super(Conv_ReLU_Block, self).__init__()\n","        # capa convolucional con filtro 3x3, padding = 1 y stride = 1\n","        # bias = false: no se agrega sesgo aprendible a la salida \n","        self.conv = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.relu = nn.ReLU(inplace=True)\n","        \n","    def forward(self, x):\n","        return self.relu(self.conv(x))\n","        \n","class VDSR(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Capa residual formada por 18 capas de convolución\n","        self.residual_layer = self.make_layer(Conv_ReLU_Block, 18)\n","\n","        # capa convolucional con filtro 3x3, padding = 1 y stride = 1\n","        # bias = false: no se agrega sesgo aprendible a la salida \n","        self.input = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n","\n","        # capa convolucional con filtro 3x3, padding = 1 y stride = 1\n","        # bias = false: no se agrega sesgo aprendible a la salida \n","        self.output = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.relu = nn.ReLU(inplace=True)\n","    \n","        for m in self.modules():\n","            # ininstance() devuelve True si el objeto especificado es del tipo especificado\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, sqrt(2. / n))\n","                \n","    # Por parámetros se pasan un bloque de convolución(conv + relu) y el número de capas, para \n","    # generar una secuencia de capas de convolución\n","    def make_layer(self, block, num_of_layer):\n","        layers = []\n","        for _ in range(num_of_layer):\n","            layers.append(block())\n","        return nn.Sequential(*layers)\n","\n","    # La entrada pasa por una ReLU, luego por la capa residual (formada por un\n","    # determinado número de bloques de convolución), por la capa de salida y,\n","    # finalmente, a la salida se suma la entrada.   \n","    def forward(self, x):\n","        residual = x\n","        out = self.relu(self.input(x))\n","        out = self.residual_layer(out)\n","        out = self.output(out)\n","        out = torch.add(out,residual)\n","        return out\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rFnd0k4ZF5bE"},"source":["# SRGAN"]},{"cell_type":"code","metadata":{"id":"RDYw-_J3F5rk","executionInfo":{"status":"ok","timestamp":1633643245326,"user_tz":-60,"elapsed":220,"user":{"displayName":"Alejandro","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08141204811401144853"}}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","from torchvision.models import vgg19\n","import math\n","\n","\n","class FeatureExtractor(nn.Module):\n","    def __init__(self):\n","        super(FeatureExtractor, self).__init__()\n","\n","        # Se carga el modelo VGG19 entrenado en el dataset de Imagenet\n","        vgg19_model = vgg19(pretrained=True)\n","\n","        # Se extraen las primeras 36 capas de salida del modelo VGG19 como content loss.\n","        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n","\n","    def forward(self, img):\n","        return self.feature_extractor(img)\n","\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_features):\n","        super(ResidualBlock, self).__init__()\n","        self.conv_block = nn.Sequential(\n","            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(in_features, 0.8), # El segundo parámetro es ‎eps, un valor añadido al denominador para la estabilidad numérica‎\n","            nn.PReLU(),\n","            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(in_features, 0.8),\n","        )\n","\n","    def forward(self, x):\n","        return x + self.conv_block(x)\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n","        super(Generator, self).__init__()\n","\n","        # Primera capa convolucional\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), \n","            nn.PReLU()\n","            )\n","\n","        # Bloques residuales\n","        res_blocks = []\n","        for _ in range(n_residual_blocks):\n","            res_blocks.append(ResidualBlock(64))\n","        self.res_blocks = nn.Sequential(*res_blocks)\n","\n","        # Segunda capa convolucional\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), \n","            nn.BatchNorm2d(64, 0.8)\n","            )\n","\n","        # Capas de Upsampling (x4)\n","        upsampling = []\n","        for _ in range(2):\n","            upsampling += [\n","                # nn.Upsample(scale_factor=2),\n","                nn.Conv2d(64, 256, 3, 1, 1),\n","                nn.BatchNorm2d(256),\n","\n","                # El número de canales se divide entre upscale_factor^2\n","                # H y W se multiplican por upscale_factor\n","                nn.PixelShuffle(upscale_factor=2),\n","                nn.PReLU(),\n","            ]\n","        self.upsampling = nn.Sequential(*upsampling)\n","\n","        # Capa de salida\n","        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())\n","\n","    def forward(self, x):\n","        out1 = self.conv1(x)\n","        out = self.res_blocks(out1)\n","        out2 = self.conv2(out)\n","        out = torch.add(out1, out2)\n","        out = self.upsampling(out)\n","        out = self.conv3(out)\n","        return out\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self,input_shape=(4,3,256,256)):\n","        super(Discriminator, self).__init__()\n","\n","        self.input_shape = input_shape\n","        _, in_channels, in_height, in_width = self.input_shape\n","        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n","        self.output_shape = (1, patch_h, patch_w)\n","\n","        def discriminator_block(in_filters, out_filters, first_block=False):\n","            layers = []\n","            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n","            if not first_block:\n","                layers.append(nn.BatchNorm2d(out_filters))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n","            layers.append(nn.BatchNorm2d(out_filters))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        layers = []\n","        in_filters = in_channels\n","        for i, out_filters in enumerate([64, 128, 256, 512]):\n","            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n","            in_filters = out_filters\n","\n","        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n","\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, img):\n","        return self.model(img)\n"],"execution_count":4,"outputs":[]}]}