{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"SRGAN.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"10lKRLil56bBL9od27uaBKm5B0_dWhR0E","authorship_tag":"ABX9TyMJk7cqTJQ7QJdrbByLFpQM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pFt-DC3cownF"},"source":["# Imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CZZjhM0aoYnp","executionInfo":{"elapsed":17733,"status":"ok","timestamp":1633689927142,"user":{"displayName":"Alejandro","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08141204811401144853"},"user_tz":-60},"outputId":"28999258-cb52-46d8-f8bd-d9cd3df78511"},"source":["# ¡¡¡ Conectar con Google Drive !!!\n","\n","# Se instala e importa import_ipynb para poder acceder a clases/funciones de otros notebooks\n","!pip install -q import-ipynb\n","\n","# Se cambia al directorio donde esté el notebook con las clases a importar\n","%cd \"/content/drive/MyDrive/Colab_Notebooks\"\n","\n","#!pip install --upgrade --force-reinstall --no-deps albumentations\n","!pip install -q -U albumentations\n","\n","!pip install -q torchmetrics"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","/content/drive/MyDrive/Colab_Notebooks\n","\u001b[K     |████████████████████████████████| 102 kB 3.7 MB/s \n","\u001b[K     |████████████████████████████████| 37.1 MB 51 kB/s \n","\u001b[K     |████████████████████████████████| 282 kB 2.8 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y9NQYnqwo0QV","executionInfo":{"elapsed":28505,"status":"ok","timestamp":1633689958561,"user":{"displayName":"Alejandro","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08141204811401144853"},"user_tz":-60},"outputId":"d07cc470-1694-4991-c66c-17ee713c47d5"},"source":["# Manejo y representación de datos\n","import numpy as np\n","import pandas as pd\n","import os \n","import matplotlib.pyplot as plt\n","import cv2\n","from tqdm import tqdm\n","\n","# Pytorch \n","import torch\n","from torch import Tensor\n","from torch.autograd import Function\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import StepLR\n","from torch.utils.tensorboard import SummaryWriter\n","\n","# Dataset y Dataloader\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset , DataLoader\n","\n","# Transformaciones\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from skimage.transform import resize\n","\n","# Modelos\n","import import_ipynb\n","from models import Generator, Discriminator, FeatureExtractor\n","\n","# Métricas\n","from torchmetrics.functional import ssim\n","from torchmetrics.functional import psnr\n","from torchmetrics.functional import mean_squared_error as mse\n","\n","\n","\n","# Device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["importing Jupyter notebook from models.ipynb\n"]}]},{"cell_type":"markdown","metadata":{"id":"3sHLM5p-pxM4"},"source":["#Rutas y dataframe"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NJC4dewQpunX","executionInfo":{"elapsed":38189,"status":"ok","timestamp":1633689999133,"user":{"displayName":"Alejandro","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08141204811401144853"},"user_tz":-60},"outputId":"e58b63c4-1e77-4708-dd7a-c254ae33386a"},"source":["img_fol= '/content/drive/MyDrive/Colab_Notebooks/carvana/train/' # Carpeta que contiene las imágenes\n","\n","# División en entrenamiento y validación\n","df = pd.DataFrame(os.listdir(img_fol), columns = ['name'])       # Se estructura de forma tabular el contenido de la carpeta 'img_fol'\n","df = df.iloc[:20]    # Se reduce el dataframe a 2000 imagenes\n","df_train, df_val = train_test_split(df, test_size=.2, random_state=42, shuffle=True, stratify=None) \n","\n","# Tras realizar la división, es necesario reiniciar los índices\n","df_train = df_train.reset_index()\n","df_val = df_val.reset_index()\n","\n","print(f\"Total: {len(df)} -- Entrenamiento: {len(df_train)} -- Validación: {len(df_val)}\")\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Total: 20 -- Entrenamiento: 16 -- Validación: 4\n"]}]},{"cell_type":"markdown","metadata":{"id":"lEPs-pKlp4T9"},"source":["#Dataset y dataloader"]},{"cell_type":"code","metadata":{"id":"rVm9B2Arp9Gb"},"source":["class CarDataset(Dataset):\n","    def __init__(self, df, transforms = None):\n","        # self.df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/carvana/train_masks.csv')\n","        self.df = df \n","        self.transforms = transforms\n","        self.img_fol= '/content/drive/MyDrive/Colab_Notebooks/carvana/train/'\n","        self.mask_fol = '/content/drive/MyDrive/Colab_Notebooks/carvana/train_masks/' \n","       \n","\n","    def __getitem__(self, idx):\n","        img_name=self.df['name'][idx]\n","        img_path=os.path.join(self.img_fol,img_name)    # Ruta completa imagen\n","\n","        img_HR = cv2.imread(img_path)      # Se carga la imagen\n","        img_HR= cv2.cvtColor(img_HR, cv2.COLOR_BGR2RGB)\n","        \n","        if self.transforms:\n","            img_HR = self.transforms(image=img_HR)['image']\n","\n","        # Factor de escala x4 en cada dimensión. Se mantiene el nº de canales\n","        img_LR = resize(img_HR.numpy(), (3,img_HR.shape[1]//4,img_HR.shape[2]//4), anti_aliasing=True)\n","        \n","        # Se realiza una interpolación bicúbica para tener las mismas dimensiones que en la imagen original\n","        # img_LR = cv2.resize(img_LR.transpose(1,2,0), (img_HR.shape[1],img_HR.shape[2]), interpolation = cv2.INTER_CUBIC).transpose(2,0,1)\n","        \n","        return img_HR, img_LR\n","\n","    def __len__(self):\n","        return len(self.df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JYTdG2Ibp_K1"},"source":["# Transformaciones\n","transforms = A.Compose([\n","            A.Resize(256, 256),\n","            A.Normalize(mean=[0.485, 0.456, 0.406], \n","                        std=[0.229, 0.224, 0.225]),\n","            ToTensorV2()\n","        ])\n","\n","# Datasets de entrenamiento y evaluación\n","carDatasetTrain = CarDataset(df=df_train, transforms=transforms)\n","carDatasetval = CarDataset(df=df_val, transforms=transforms)\n","\n","# Dataloaders de entrenamiento y evaluación\n","train_dataloader = DataLoader(carDatasetTrain, batch_size=4,\n","                        shuffle=True, num_workers=0)\n","val_dataloader = DataLoader(carDatasetval, batch_size=4,\n","                        shuffle=False, num_workers=0) # El dataloader de validación es fijo, no debe tener shuffle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3SjXwhR36zrH"},"source":["img_HR, img_LR = next(iter(train_dataloader))\n","print(img_HR.shape, img_LR.shape)\n","bs = img_HR.shape[0]\n","plt.figure(figsize=(15,15))\n","print(img_LR.shape)\n","for i in range(bs):\n","    plt.subplot(bs,2,(2*i+1))\n","    plt.title(f\"Imagen con transformaciones\")\n","    plt.imshow(img_HR[i].numpy().transpose(1,2,0))\n","    plt.subplot(bs,2,(2*i+2))\n","    plt.title(f\"Imagen submuestreada\")\n","    plt.imshow(img_LR[i].squeeze().numpy().transpose(1,2,0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["029d9381d8cb4603853d108c40008798"]},"id":"TEgNpVA1E-vV","executionInfo":{"elapsed":23232,"status":"ok","timestamp":1633690048280,"user":{"displayName":"Alejandro","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08141204811401144853"},"user_tz":-60},"outputId":"048d28c5-62b8-4e36-92ef-c03605ce6458"},"source":["# Se instancian los modelos y se envían a la gpu si está disponible\n","generator = Generator().to(device)                # Generador\n","discriminator = Discriminator().to(device)        # Discriminador\n","feature_extractor = FeatureExtractor().to(device) # \n","\n","# Funciones de pérdida\n","criterion_GAN = torch.nn.MSELoss().to(device)\n","criterion_content = torch.nn.L1Loss().to(device)\n","\n","# Optimizers\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","cuda = torch.cuda.is_available()\n","Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n"],"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"029d9381d8cb4603853d108c40008798","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"id":"_Kd5W3AnImvi"},"source":["def train(epoch, epochs, tloader):\n","\n","    # Se establecen los modelos en modo entrenamiento\n","    generator.train()\n","    discriminator.train()\n","    feature_extractor.train()\n","\n","    n_train = len(train_dataloader) # Se calcula el número de batches de entrenamiento\n","\n","    tloader.set_description(f'EPOCH {epoch+1}')\n","\n","    # Se inicializan a 0 las variables que almacenarán las métricas   \n","    # obtenidas en cada batch\n","    epoch_loss_G, epoch_loss_D, epoch_psnr, epoch_ssim, epoch_rmse = 0,0,0,0,0\n","\n","    for hr, lr in tloader:\n","        # Los batches de imágenes se envían a la GPU si está disponible\n","        hr, lr = hr.to(device), lr.to(device)\n","\n","        # Adversarial ground truth\n","        valid = Tensor(np.ones((lr.size(0), *discriminator.output_shape)))\n","        fake = Tensor(np.zeros((lr.size(0), *discriminator.output_shape)))\n","\n","        #----------------------------\n","        # Entrenamiento del generador\n","        #----------------------------\n","\n","        # Se ponen a cero los gradientes del generador\n","        optimizer_G.zero_grad()\n","\n","        # Se obtiene la imagen SR\n","        sr = generator(lr)\n","\n","        # Adversarial loss\n","        loss_GAN = criterion_GAN(discriminator(sr),valid)\n","\n","        # Content loss\n","        gen_features = feature_extractor(sr)\n","        real_features = feature_extractor(hr)\n","        loss_content = criterion_content(gen_features, real_features.detach()) \n","\n","        # Total loss\n","        loss_G = loss_content + 1e-3 * loss_GAN\n","\n","        # Back propagation\n","        loss_G.backward()\n","\n","        # Actualización de los pesos del generador\n","        optimizer_G.step()   \n","\n","        #-------------------------------\n","        # Entrenamiento del discrminador\n","        #-------------------------------\n","\n","        # Se ponen a cero los gradientes del discriminador\n","        optimizer_D.zero_grad()\n","\n","        # Loss de las imágenes hr y sr\n","        loss_real = criterion_GAN(discriminator(hr), valid)\n","        loss_fake = criterion_GAN(discriminator(sr.detach()), fake)\n","\n","        # Total loss\n","        loss_D = (loss_real + loss_fake) / 2\n","\n","        # Back propagation\n","        loss_D.backward()\n","\n","        # Actualización de los pesos del discriminador\n","        optimizer_D.step()\n","\n","        #---------\n","        # Métricas\n","        #---------\n","\n","        # Losses\n","        epoch_loss_G += loss_G\n","        epoch_loss_D += loss_D\n","\n","        # Se calcula la PSNR\n","        psnr_score = psnr(sr,hr)\n","        epoch_psnr += psnr_score.item()      \n","\n","        # Se calcula el SSIM\n","        ssim_score = ssim(sr, hr)\n","        epoch_ssim += ssim_score.item()   \n","\n","        # Se calcula el RSME\n","        rmse_score = mse(sr,hr,squared = True)\n","        epoch_rmse += rmse_score.item()   \n","\n","        tloader.set_postfix(loss_G=loss_G.item(),loss_D=loss_D.item(), psnr=psnr_score.item(), ssim=ssim_score.item(), rmse=rmse_score.item())\n","         \n","    print(f'Train Loss_G Epoch : {(epoch_loss_G/n_train)}')\n","    print(f'Train Loss_D Epoch : {(epoch_loss_D/n_train)}')\n","    print(f'Train PSNR Epoch : {(epoch_psnr/n_train)}')\n","    print(f'Train SSIM Epoch : {(epoch_ssim/n_train)}')\n","    print(f'Train RMSE Epoch : {(epoch_rmse/n_train)}')\n","\n","    # Se devuelve el valor medio de cada métrica\n","    return [(epoch_loss_G/n_train),(epoch_loss_D/n_train),(epoch_psnr/n_train),(epoch_ssim/n_train),(epoch_rmse/n_train)]\n","\n","\n","def validation(vloader):\n","\n","    # Se establecen los modelos en modo evaluación\n","    generator.eval()\n","    discriminator.eval()\n","    feature_extractor.eval()\n","\n","    n_val = len(val_dataloader) # Se calcula el número de batches de evaluación\n","\n","    vloader.set_description(f'Validation')\n","\n","    # Se inicializan a 0 las variables que almacenarán las métricas   \n","    # obtenidas en cada batch\n","    epoch_loss_G, epoch_loss_D, epoch_psnr, epoch_ssim, epoch_rmse = 0,0,0,0,0\n","\n","    # Contador del nbatches transcurridos\n","    nbatch = 0\n","    with torch.no_grad():\n","        for hr, lr in tloader:\n","              nbatch += 1\n","             \n","              # Los batches de imágenes se envían a la GPU si está disponible\n","              hr, lr = hr.to(device), lr.to(device)\n","\n","              # Adversarial ground truth\n","              valid = Tensor(np.ones((lr.size(0), *discriminator.output_shape)))\n","              fake = Tensor(np.zeros((lr.size(0), *discriminator.output_shape)))\n","\n","              #----------------------------\n","              # Entrenamiento del generador\n","              #----------------------------\n","\n","              # Se obtiene la imagen SR\n","              sr = generator(lr)\n","\n","              # Adversarial loss\n","              loss_GAN = criterion_GAN(discriminator(sr),valid)\n","\n","              # Content loss\n","              gen_features = feature_extractor(sr)\n","              real_features = feature_extractor(hr)\n","              loss_content = criterion_content(gen_features, real_features.detach()) \n","\n","              # Total loss\n","              loss_G = loss_content + 1e-3 * loss_GAN  \n","\n","              #-------------------------------\n","              # Entrenamiento del discrminador\n","              #-------------------------------\n","\n","              # Loss de las imágenes hr y sr\n","              loss_real = criterion_GAN(discriminator(hr), valid)\n","              loss_fake = criterion_GAN(discriminator(sr.detach()), fake)\n","\n","              # Total loss\n","              loss_D = (loss_real + loss_fake) / 2\n","\n","              #---------\n","              # Métricas\n","              #---------\n","\n","              # Losses\n","              epoch_loss_G += loss_G\n","              epoch_loss_D += loss_D\n","\n","              # Se calcula la PSNR\n","              psnr_score = psnr(sr,hr)\n","              epoch_psnr += psnr_score.item()      \n","\n","              # Se calcula el SSIM\n","              ssim_score = ssim(sr, hr)\n","              epoch_ssim += ssim_score.item()   \n","\n","              # Se calcula el RSME\n","              rmse_score = mse(sr,hr,squared = True)\n","              epoch_rmse += rmse_score.item()   \n","\n","              vloader.set_postfix(loss_G=loss_G.item(),loss_D=loss_D.item(), psnr=psnr_score.item(), ssim=ssim_score.item(), rmse=rmse_score.item())\n","\n","              if nbatch == 1:\n","                  plt.figure(figsize=(15,15))\n","                  plt.suptitle(f\"Epoch: {epoch+1}\\n\"\n","                              +f\"SR:    RMSE: {rmse_score:.2f} -- PSNR: {psnr_score:.2f} -- SSIM: {ssim_score:.2f}\\n\",\n","                              fontsize=16, y=0.96)\n","                  \n","                  # plt.suptitle(f\"Epoch: {epoch+1}\\n\"\n","                  #             +f\"SR:    RMSE: {rmse_score:.2f} -- PSNR: {psnr_score:.2f} -- SSIM: {ssim_score:.2f}\\n\"\n","                  #             +f\"LR:    RMSE: {mse(lr,hr,squared = True):.2f} -- PSNR: {psnr(lr,hr):.2f} -- SSIM: {ssim(lr, hr):.2f}\", \n","                  #             fontsize=16, y=0.96)                  \n","                  for i in range(bs):\n","                      plt.subplot(bs,3,(3*i+1))\n","                      plt.title(f\"HR\")\n","                      plt.imshow(hr[i].cpu().numpy().transpose(1,2,0))\n","                      plt.subplot(bs,3,(3*i+2))\n","                      plt.title(f\"LR\")\n","                      plt.imshow(lr[i].cpu().numpy().transpose(1,2,0))\n","                      plt.subplot(bs,3,(3*i+3))\n","                      plt.title(f\"SR\")\n","                      plt.imshow(sr[i].cpu().detach().numpy().transpose(1,2,0))\n","              \n","\n","\n","    print(f'Train Loss_G Epoch : {(epoch_loss_G/n_val)}')\n","    print(f'Train Loss_D Epoch : {(epoch_loss_D/n_val)}')\n","    print(f'Train PSNR Epoch : {(epoch_psnr/n_val)}')\n","    print(f'Train SSIM Epoch : {(epoch_ssim/n_val)}')\n","    print(f'Train RMSE Epoch : {(epoch_rmse/n_val)}')\n","\n","    # Se devuelve el valor medio de cada métrica\n","    return [(epoch_loss_G/n_val),(epoch_loss_D/n_val),(epoch_psnr/n_val),(epoch_ssim/n_val),(epoch_rmse/n_val)]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"wQFhLQxPP7uP"},"source":["epochs = 10\n","bs = 4\n","# Se inicializan listas vacias en las que se almacenarán lás metricas de\n","# entrenamiento para cada epoch \n","train_loss_G,train_loss_D,train_psnr, train_ssim, train_rmse = [], [], [], [], []\n","\n","# Se inicializan listas vacias en las que se almacenarán lás metricas de\n","# validación para cada epoch\n","val_loss_G, val_loss_D, val_psnr, val_ssim, val_rmse = [], [], [], [], []\n","\n","\n","for epoch in range(epochs) :\n","\n","    print(epoch+1,'/',epochs)\n","\n","    with tqdm(train_dataloader,unit='batch') as tloader : \n","        train_scores = train(epoch,epochs,tloader)\n","\n","        # Se almacenan las métricas en las listas correspondientes\n","        train_loss_G.append(train_scores[0])\n","        train_loss_D.append(train_scores[1])\n","        train_psnr.append(train_scores[2])\n","        train_ssim.append(train_scores[3])\n","        train_rmse.append(train_scores[4])\n","\n","    with tqdm(val_dataloader,unit='batch') as vloader : \n","        val_scores = validation(vloader)\n","\n","        # Se almacenan las métricas en las listas correspondientes\n","        val_loss_G.append(val_scores[0])\n","        val_loss_D.append(val_scores[1])\n","        val_psnr.append(val_scores[2])\n","        val_ssim.append(val_scores[3])\n","        val_rmse.append(val_scores[4])\n","\n","        if epoch == 0:\n","            best_psnr_value = 0  \n","        is_best = val_ssim[epoch] > best_psnr_value\n","        best_psnr_value = max(val_ssim[epoch], best_psnr_value)\n","        if is_best:\n","            torch.save(generator.state_dict(), \"/content/drive/MyDrive/Colab_Notebooks/saved_models/generator.pth\")\n","            torch.save(discriminator.state_dict(), \"/content/drive/MyDrive/Colab_Notebooks/saved_models/discriminator.pth\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7GkzAlumdUdb"},"source":["plt.figure(figsize=(15,5))\n","plt.subplot(1,5,1)\n","plt.title(f\"Loss_G\")\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss_G')\n","plt.plot(train_loss_G, label='train')\n","plt.plot(val_loss_G, label='validation')\n","plt.legend()\n","\n","plt.subplot(1,5,2)\n","plt.title(f\"Loss_D\")\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss_D')\n","plt.plot(train_loss_D, label='train')\n","plt.plot(val_loss_G, label='validation')\n","plt.legend()\n","\n","plt.subplot(1,5,3)\n","plt.title(f\"RMSE\")\n","plt.xlabel('Epoch')\n","plt.ylabel('RMSE')\n","plt.plot(train_rmse, label='train')\n","plt.plot(val_rmse, label='validation')\n","plt.legend()\n","\n","plt.subplot(1,5,4)\n","plt.title(f\"PSNR\")\n","plt.xlabel('Epoch')\n","plt.ylabel('PSNR')\n","plt.plot(train_psnr, label='train')\n","plt.plot(val_psnr, label='validation')\n","plt.legend()\n","\n","\n","plt.subplot(1,5,5)\n","plt.title(f\"SSIM\")\n","plt.xlabel('Epoch')\n","plt.ylabel('SSIM')\n","plt.plot(train_ssim, label='train')\n","plt.plot(val_ssim, label='validation')\n","plt.legend()\n"],"execution_count":null,"outputs":[]}]}