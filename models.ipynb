{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77fcmjqJ1lXj"
      },
      "source": [
        "##UNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEXR5eCw0jrW"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def conv3x3_bn(ci, co):\n",
        "    return torch.nn.Sequential(\n",
        "        # capa convolucional con filtro 3x3 y padding = 1, por lo que no cambian las dimensiones\n",
        "        torch.nn.Conv2d(ci, co, 3, padding=1), # ci = canales de entrada; co 0 canales de salida\n",
        "        torch.nn.BatchNorm2d(co),\n",
        "        torch.nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "def encoder_conv(ci, co):\n",
        "  return torch.nn.Sequential(\n",
        "        torch.nn.MaxPool2d(2),  # max pool 2x2\n",
        "        conv3x3_bn(ci, co),     # conv 3x3, ReLU\n",
        "        conv3x3_bn(co, co),     # conv 3x3, ReLU\n",
        "    )\n",
        "\n",
        "class deconv(torch.nn.Module):\n",
        "    def __init__(self, ci, co):\n",
        "        super(deconv, self).__init__()\n",
        "\n",
        "        # upconv 2x2 (convolución traspuesta 2D)\n",
        "        self.upsample = torch.nn.ConvTranspose2d(ci, co, 2, stride=2) # kernel de tamaño 2x2 y stride = 2 \n",
        "        # con kernel 2x2 y stride = 2 se obtiene un mapa de características del doble de resolución\n",
        "\n",
        "        self.conv1 = conv3x3_bn(ci, co) # conv 3x3, ReLU\n",
        "        self.conv2 = conv3x3_bn(co, co) # conv 3x3, ReLU\n",
        "    \n",
        "    # recibe la salida de la capa anterior(x1) y la salida de la etapa\n",
        "    # correspondiente del encoder (x2)\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.upsample(x1)  # A la salida de la capa anterior se aplica convolución traspuesta\n",
        "\n",
        "        # Para evitar inconcordancias en el tamaño y que las dimensiones sean las mismas\n",
        "        diffX = x2.size()[2] - x1.size()[2]\n",
        "        diffY = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, (diffX, 0, diffY, 0))\n",
        "\n",
        "        # concatenamos los tensores\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# n_classes número de clases diferentes\n",
        "# in_ch número de canales de la imagen de entrada\n",
        "class UNet(torch.nn.Module):\n",
        "    def __init__(self, n_classes=3, in_ch=1): \n",
        "        super().__init__()\n",
        "\n",
        "        # lista de capas en encoder-decoder con número de filtros\n",
        "        c = [16, 32, 64, 128] # enconder en este orden; decoder en orden inverso\n",
        "\n",
        "        # primera capa conv que recibe la imagen\n",
        "        self.conv1 = torch.nn.Sequential(\n",
        "          conv3x3_bn(in_ch, c[0]),\n",
        "          conv3x3_bn(c[0], c[0]),\n",
        "        )\n",
        "        # capas del encoder\n",
        "        self.conv2 = encoder_conv(c[0], c[1])\n",
        "        self.conv3 = encoder_conv(c[1], c[2])\n",
        "        self.conv4 = encoder_conv(c[2], c[3])\n",
        "\n",
        "        # capas del decoder\n",
        "        self.deconv1 = deconv(c[3],c[2])\n",
        "        self.deconv2 = deconv(c[2],c[1])\n",
        "        self.deconv3 = deconv(c[1],c[0])\n",
        "\n",
        "        # útlima capa conv que nos da la máscara\n",
        "        # el número de filtros a la entrada es el primer valor de la lista\n",
        "        # se aplican tantos filtros como clases haya \n",
        "        self.out = torch.nn.Conv2d(c[0], n_classes, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoder\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.conv2(x1)\n",
        "        x3 = self.conv3(x2)\n",
        "        x = self.conv4(x3)\n",
        "        # decoder\n",
        "        x = self.deconv1(x, x3)\n",
        "        x = self.deconv2(x, x2)\n",
        "        x = self.deconv3(x, x1)\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbAHP0-91qO9"
      },
      "source": [
        "##UNet-ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XagXZhsH1pMq"
      },
      "source": [
        "class out_conv(torch.nn.Module):\n",
        "    def __init__(self, ci, co, coo):\n",
        "        super(out_conv, self).__init__()\n",
        "\n",
        "        # upconv 2x2 (convolución traspuesta 2D)\n",
        "        self.upsample = torch.nn.ConvTranspose2d(ci, co, 2, stride=2) # kernel de tamaño 2x2 y stride = 2\n",
        "        # con kernel 2x2 y stride = 2 se obtiene un mapa de características del doble de resolución\n",
        "\n",
        "        self.conv = conv3x3_bn(ci, co)  # conv 3x3, ReLU\n",
        "        self.final = torch.nn.Conv2d(co, coo, 1) # A la salida se obtiene el número de clases necesario\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.upsample(x1)\n",
        "        diffX = x2.size()[2] - x1.size()[2]\n",
        "        diffY = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, (diffX, 0, diffY, 0))\n",
        "        x = self.conv(x1)\n",
        "        x = self.final(x)\n",
        "        return x\n",
        "\n",
        "class UNetResnet(torch.nn.Module):\n",
        "    def __init__(self, n_classes=3, in_ch=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # En lugar de utilizar un encoder propio, se utiliza una red entrenada: ResNet18\n",
        "        self.encoder = torchvision.models.resnet18(pretrained=True)  \n",
        "\n",
        "        # ResNet18 está diseñada para trabajar con 3 canaless, pero nuestras imagenes son en ByN \n",
        "        # por lo que es necesasrio modificar la primera capa    \n",
        "        if in_ch != 3:\n",
        "          self.encoder.conv1 = torch.nn.Conv2d(in_ch, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "\n",
        "        self.deconv1 = deconv(512,256)\n",
        "        self.deconv2 = deconv(256,128)\n",
        "        self.deconv3 = deconv(128,64)\n",
        "\n",
        "        # Capa que genera los mapas de salidas. Estos tienen que tener tantos canales \n",
        "        # como número de clases\n",
        "        self.out = out_conv(64, 64, n_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = torch.tensor(x.clone())\n",
        "        x = self.encoder.relu(self.encoder.bn1(self.encoder.conv1(x)))\n",
        "        x1 = self.encoder.layer1(x)\n",
        "        x2 = self.encoder.layer2(x1)\n",
        "        x3 = self.encoder.layer3(x2)\n",
        "        x = self.encoder.layer4(x3)\n",
        "        x = self.deconv1(x, x3)\n",
        "        x = self.deconv2(x, x2)\n",
        "        x = self.deconv3(x, x1)\n",
        "        x = self.out(x, x_in)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGFG9_PJdUC4"
      },
      "source": [
        "# VDSR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnRftTUPdTaI"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from math import sqrt\n",
        "\n",
        "class Conv_ReLU_Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Conv_ReLU_Block, self).__init__()\n",
        "        # capa convolucional con filtro 3x3, padding = 1 y stride = 1\n",
        "        # bias = false: no se agrega sesgo aprendible a la salida \n",
        "        self.conv = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.relu(self.conv(x))\n",
        "        \n",
        "class VDSR(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa residual formada por 18 capas de convolución\n",
        "        self.residual_layer = self.make_layer(Conv_ReLU_Block, 18)\n",
        "\n",
        "        # capa convolucional con filtro 3x3, padding = 1 y stride = 1\n",
        "        # bias = false: no se agrega sesgo aprendible a la salida \n",
        "        self.input = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # capa convolucional con filtro 3x3, padding = 1 y stride = 1\n",
        "        # bias = false: no se agrega sesgo aprendible a la salida \n",
        "        self.output = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "    \n",
        "        for m in self.modules():\n",
        "            # ininstance() devuelve True si el objeto especificado es del tipo especificado\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, sqrt(2. / n))\n",
        "                \n",
        "    # Por parámetros se pasan un bloque de convolución(conv + relu) y el número de capas, para \n",
        "    # generar una secuencia de capas de convolución\n",
        "    def make_layer(self, block, num_of_layer):\n",
        "        layers = []\n",
        "        for _ in range(num_of_layer):\n",
        "            layers.append(block())\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    # La entrada pasa por una ReLU, luego por la capa residual (formada por un\n",
        "    # determinado número de bloques de convolución), por la capa de salida y,\n",
        "    # finalmente, a la salida se suma la entrada.   \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.input(x))\n",
        "        out = self.residual_layer(out)\n",
        "        out = self.output(out)\n",
        "        out = torch.add(out,residual)\n",
        "        return out\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFnd0k4ZF5bE"
      },
      "source": [
        "# SRGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDYw-_J3F5rk"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torch import Tensor\n",
        "\n",
        "class ResidualConvBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualConvBlock, self).__init__()\n",
        "        self.rc_block  = nn.Sequential(\n",
        "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(in_features), # El segundo parámetro es ‎eps, un valor añadido al denominador para la estabilidad numérica‎\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(in_features),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.rc_block (x)\n",
        "        out = out + identity\n",
        "        return out\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Primera capa convolucional\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=9, stride=1, padding=4), \n",
        "            nn.PReLU()\n",
        "            )\n",
        "\n",
        "        # Bloques residuales\n",
        "        res_blocks = []\n",
        "        for _ in range(16):\n",
        "            res_blocks.append(ResidualConvBlock(64))\n",
        "        self.res_blocks = nn.Sequential(*res_blocks)\n",
        "\n",
        "        # Segunda capa convolucional\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False), \n",
        "            nn.BatchNorm2d(64)\n",
        "            )\n",
        "\n",
        "        # Bloque convolución upsampling (x4)\n",
        "        self.upsampling = nn.Sequential(\n",
        "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.PixelShuffle(2),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.PixelShuffle(2),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "\n",
        "        # Capa de salida\n",
        "        self.conv3 = nn.Conv2d(64, 3, kernel_size=9, stride=1, padding=4)\n",
        "\n",
        "        # Se inicializa los pesos \n",
        "        self.initialize_weights()  \n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.forward_impl(x)\n",
        "\n",
        "    def forward_impl(self, x):\n",
        "        out1 = self.conv1(x)\n",
        "        out = self.res_blocks(out1)\n",
        "        out2 = self.conv2(out)\n",
        "        out = out1 + out2\n",
        "        out = self.upsampling(out)\n",
        "        out = self.conv3(out)\n",
        "        return out\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "                m.weight.data *= 0.1\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                m.weight.data *= 0.1\n",
        "\n",
        "class ContentLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ContentLoss, self).__init__()\n",
        "\n",
        "        # Se carga el modelo VGG19 preentrenador.\n",
        "        vgg19 = models.vgg19(pretrained=True, num_classes=1000).eval()\n",
        "\n",
        "        # Se extraen las primeras 36 capas del modelo VGG19 como content loss\n",
        "        self.feature_extractor = nn.Sequential(*list(vgg19.features.children())[:36])\n",
        "\n",
        "        # Se congelan los parámetros del modelo\n",
        "        for parameters in self.feature_extractor.parameters():\n",
        "            parameters.requires_grad = False\n",
        "\n",
        "        # The preprocessing method of the input data. This is the VGG model preprocessing method of the ImageNet dataset.\n",
        "        # self.register_buffer(\"mean\", torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "        # self.register_buffer(\"std\", torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "\n",
        "    def forward(self, sr, hr):\n",
        "        # Standardized operations.\n",
        "        # sr = (sr - self.mean) / self.std\n",
        "        # hr = (hr - self.mean) / self.std\n",
        "\n",
        "        # Find the feature map difference between the two images.\n",
        "        loss = F.mse_loss(self.feature_extractor(sr), self.feature_extractor(hr))\n",
        "\n",
        "        return loss    \n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # input size. (3) x 96 x 96\n",
        "            nn.Conv2d(3, 64, (3, 3), (1, 1), (1, 1), bias=True),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            # state size. (64) x 48 x 48\n",
        "            nn.Conv2d(64, 64, (3, 3), (2, 2), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(64, 128, (3, 3), (1, 1), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            # state size. (128) x 24 x 24\n",
        "            nn.Conv2d(128, 128, (3, 3), (2, 2), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(128, 256, (3, 3), (1, 1), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            # state size. (256) x 12 x 12\n",
        "            nn.Conv2d(256, 256, (3, 3), (2, 2), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(256, 512, (3, 3), (1, 1), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            # state size. (512) x 6 x 6\n",
        "            nn.Conv2d(512, 512, (3, 3), (2, 2), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512*16*16, 1024),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Linear(1024, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}